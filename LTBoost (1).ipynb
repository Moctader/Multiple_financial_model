{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed606f1-a4a3-42bd-9069-d401d3f78712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gdown in /home/wickstjo/.local/lib/python3.10/site-packages (5.2.0)\n",
      "Requirement already satisfied: requests[socks] in /home/wickstjo/.local/lib/python3.10/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from gdown) (3.6.0)\n",
      "Requirement already satisfied: tqdm in /home/wickstjo/.local/lib/python3.10/site-packages (from gdown) (4.66.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/wickstjo/.local/lib/python3.10/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/wickstjo/.local/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wickstjo/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wickstjo/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wickstjo/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (3.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/wickstjo/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lightgbm in /home/wickstjo/.local/lib/python3.10/site-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/wickstjo/.local/lib/python3.10/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /home/wickstjo/.local/lib/python3.10/site-packages (from lightgbm) (1.14.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/wickstjo/.local/lib/python3.10/site-packages (2.4.1)\n",
      "Requirement already satisfied: sympy in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: networkx in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: fsspec in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/wickstjo/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/wickstjo/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/wickstjo/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install gdown\n",
    "! pip install lightgbm\n",
    "! pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4454f6-de95-4b03-8cd8-fe6258187a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/wickstjo/.local/lib/python3.10/site-packages (2.4.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: fsspec in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: networkx in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: sympy in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/wickstjo/.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/wickstjo/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/wickstjo/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/wickstjo/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee228f9b-c054-4120-85e8-904ed1a8251c",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d7c976-4eb7-4e1a-9902-a2bcdbb6dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "fix_seed = 2021\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700ab92-1926-40f5-a668-c7a5a22509a9",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b1b0c93-6697-4492-b588-bb06020eaaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DS_NAME = 'EODHD_EURUSD_HISTORICAL_2019_2024_1min.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6072e7d2-e2f1-4832-8df4-1cbdaa25558b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             timestamp     open     high      low    close  volume\n",
      "0  2019-01-02 00:25:00  1.14573  1.14574  1.14565  1.14565     128\n",
      "1  2019-01-02 00:26:00  1.14565  1.14571  1.14564  1.14568      97\n",
      "2  2019-01-02 00:27:00  1.14567  1.14581  1.14562  1.14578      88\n",
      "3  2019-01-02 00:28:00  1.14577  1.14579  1.14572  1.14575      52\n",
      "4  2019-01-02 00:29:00  1.14575  1.14581  1.14573  1.14579      71\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DS_NAME)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a168366e-e517-440a-a1fd-bffee741e54d",
   "metadata": {},
   "source": [
    "# Define all the relevant exp vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe85e8b-012d-4b1f-bfef-e1b11105f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 104\n",
    "LABEL_LEN = 18\n",
    "PRED_LEN = 60\n",
    "ENC_IN = 4\n",
    "INDIVIDUAL = False\n",
    "LR = 0.01\n",
    "BATCH_SIZE = 32\n",
    "TREE_LR = 0.05\n",
    "TREE_LOSS = 'Mixed'\n",
    "TREE_LB = 104\n",
    "TREE_ITER = 100\n",
    "NUM_LEAVES = 3\n",
    "PSMOOTH = 15\n",
    "LB_DATA = 'N'\n",
    "NUM_JOBS = 1\n",
    "USE_GPU = False\n",
    "DEVICE = 'cpu'\n",
    "NORMALIZE = True\n",
    "USE_REVIN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fc9d1f-9666-4265-923d-db12cf5f1c40",
   "metadata": {},
   "source": [
    "# Define RevIN and LTBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ef103b6-3772-4f5c-a784-0a83f986fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features: int, eps=1e-5, affine=True, subtract_last=False):\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        self.subtract_last = subtract_last\n",
    "        if self.affine:\n",
    "            self._init_params()\n",
    "\n",
    "    def forward(self, x, mode: str):\n",
    "        if mode == 'norm':\n",
    "            self._get_statistics(x)\n",
    "            x = self._normalize(x)\n",
    "        elif mode == 'denorm':\n",
    "            x = self._denormalize(x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x\n",
    "\n",
    "    def _init_params(self):\n",
    "        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n",
    "\n",
    "    def _get_statistics(self, x):\n",
    "        dim2reduce = tuple(range(1, x.ndim - 1))\n",
    "        if self.subtract_last:\n",
    "            self.last = x[:, -1, :].unsqueeze(1)\n",
    "        else:\n",
    "            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n",
    "        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n",
    "\n",
    "    def _normalize(self, x):\n",
    "        if self.subtract_last:\n",
    "            x = x - self.last\n",
    "        else:\n",
    "            x = x - self.mean\n",
    "        x = x / self.stdev\n",
    "        if self.affine:\n",
    "            x = x * self.affine_weight\n",
    "            x = x + self.affine_bias\n",
    "        return x\n",
    "\n",
    "    def _denormalize(self, x):\n",
    "        if self.affine:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + self.eps * self.eps)\n",
    "        x = x * self.stdev\n",
    "        if self.subtract_last:\n",
    "            x = x + self.last\n",
    "        else:\n",
    "            x = x + self.mean\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.seq_len = SEQ_LEN\n",
    "        self.pred_len = PRED_LEN\n",
    "        self.channels = ENC_IN\n",
    "        self.individual = INDIVIDUAL\n",
    "\n",
    "        self.Tree = []\n",
    "        self.tree_lr = TREE_LR\n",
    "        self.tree_loss = TREE_LOSS\n",
    "        self.treelb = min(TREE_LB, SEQ_LEN)\n",
    "        self.lb_data = LB_DATA\n",
    "        self.num_leaves = NUM_LEAVES\n",
    "        self.tree_iter = TREE_ITER\n",
    "        self.psmooth = PSMOOTH\n",
    "        self.num_jobs = NUM_JOBS\n",
    "\n",
    "        self.device = 'cuda' if USE_GPU else 'cpu'\n",
    "        self.normalize = NORMALIZE\n",
    "        self.use_RevIN = USE_REVIN\n",
    "\n",
    "        if self.individual:\n",
    "            self.Linear = nn.ModuleList()\n",
    "            for i in range(self.channels):\n",
    "                self.Linear.append(nn.Linear(self.seq_len, self.pred_len))\n",
    "        else:\n",
    "            self.Linear = nn.Linear(self.seq_len, self.pred_len)\n",
    "\n",
    "        if self.use_RevIN:\n",
    "            self.revin = RevIN(self.channels)\n",
    "\n",
    "    def forward(self, X):\n",
    "        seq_last = X[:, -1:, :]\n",
    "        X = X - seq_last\n",
    "\n",
    "        if self.normalize:\n",
    "            seq_last = X[:, -1:, :].detach()\n",
    "            X = X - seq_last\n",
    "        if self.use_RevIN:\n",
    "            X = self.revin(X, 'norm')\n",
    "\n",
    "        output = torch.zeros([X.size(0), self.pred_len, X.size(2)], dtype=X.dtype).to(X.device)\n",
    "        if self.individual:\n",
    "            for i in range(self.channels):\n",
    "                output[:, :, i] = self.Linear[i](X[:, :, i])\n",
    "        else:\n",
    "            output = self.Linear(X.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "        if self.use_RevIN:\n",
    "            output = self.revin(output, 'denorm')\n",
    "        if self.normalize:\n",
    "            output = output + seq_last\n",
    "\n",
    "        return output\n",
    "\n",
    "    def train(self, X, y):\n",
    "        seq_last = X[:, -1:, :]\n",
    "        X = X - seq_last\n",
    "        print(f\"Initial X shape: {X.shape}\")\n",
    "        print(f\"Initial y shape: {y.shape}\")\n",
    "\n",
    "        y = y.unsqueeze(-1)  # Ensure y has the same shape as seq_last\n",
    "        y = y.expand(-1, -1, seq_last.size(2))  # Ensure y has the same number of channels as seq_last\n",
    "        print(f\"y after unsqueeze and expand shape: {y.shape}\")\n",
    "\n",
    "        y = y - seq_last\n",
    "        print(f\"y after subtraction shape: {y.shape}\")\n",
    "\n",
    "        output = torch.zeros([X.size(0), self.pred_len, X.size(2)], dtype=X.dtype).to(X.device)\n",
    "        with torch.no_grad():\n",
    "            if self.use_RevIN:\n",
    "                l_in = self.revin(X, 'norm')\n",
    "\n",
    "            lin_in = X if not self.use_RevIN else l_in\n",
    "            if self.individual:\n",
    "                for i in range(self.channels):\n",
    "                    output[:, :, i] = self.Linear[i](lin_in[:, :, i].float())\n",
    "            else:\n",
    "                output = self.Linear(lin_in.permute(0, 2, 1).float()).permute(0, 2, 1)\n",
    "\n",
    "            if self.use_RevIN:\n",
    "                output = self.revin(output, 'denorm')\n",
    "\n",
    "        print(f\"output shape: {output.shape}\")\n",
    "\n",
    "        y = y - output\n",
    "        print(f\"y after final subtraction shape: {y.shape}\")\n",
    "\n",
    "        if self.treelb > 0:\n",
    "            if self.lb_data == '0':\n",
    "                if self.normalize:\n",
    "                    X += seq_last\n",
    "                X = torch.cat((X[:, -self.treelb:, :], output), dim=1)\n",
    "            elif self.lb_data == 'N':\n",
    "                X = torch.cat((X[:, -self.treelb:, :], output), dim=1)\n",
    "            else:\n",
    "                X = torch.cat((lin_in[:, -self.treelb:, :], output), dim=1)\n",
    "        else:\n",
    "            X = output\n",
    "\n",
    "        X, y = X.cpu().detach().numpy(), y.cpu().detach().numpy()\n",
    "\n",
    "        self.Tree = []\n",
    "        for i in range(self.channels):\n",
    "            dtrain = lgb.Dataset(X[:, :, i])\n",
    "\n",
    "            def multi_mse(y_hat, dtrain):\n",
    "                y_true = y[:, :, i]\n",
    "                grad = y_hat - y_true\n",
    "                hess = np.ones_like(y_true)\n",
    "                return grad.flatten(\"F\"), hess.flatten(\"F\")\n",
    "\n",
    "            def pseudo_huber(y_hat, dtrain):\n",
    "                y_true = y[:, :, i]\n",
    "                d = (y_hat - y_true)\n",
    "                h = 1  # h is delta, 1 = huber loss\n",
    "                scale = 1 + (d / h) ** 2\n",
    "                scale_sqrt = np.sqrt(scale)\n",
    "                grad = d / scale_sqrt\n",
    "                hess = 1 / scale / scale_sqrt\n",
    "                return grad, hess\n",
    "\n",
    "            def mixed_loss(y_hat, dtrain):\n",
    "                y_true = y[:, :, i]\n",
    "                grad1 = y_hat - y_true\n",
    "                hess1 = np.ones_like(y_true)\n",
    "\n",
    "                scale = 1 + grad1 ** 2\n",
    "                scale_sqrt = np.sqrt(scale)\n",
    "                grad2 = grad1 / scale_sqrt\n",
    "                hess2 = 1 / scale / scale_sqrt\n",
    "                return 0.5 * (grad1 + grad2), 0.5 * (hess1 + hess2)\n",
    "\n",
    "            if self.tree_loss == 'Huber':\n",
    "                loss_func = pseudo_huber\n",
    "            elif self.tree_loss == 'Mixed':\n",
    "                loss_func = mixed_loss\n",
    "            else:\n",
    "                loss_func = multi_mse\n",
    "\n",
    "            self.Tree.append(\n",
    "                lgb.train(\n",
    "                    train_set=dtrain,\n",
    "                    params={\n",
    "                        \"boosting\": \"gbdt\",\n",
    "                        \"objective\": loss_func,\n",
    "                        \"num_class\": self.pred_len,\n",
    "                        \"num_threads\": self.num_jobs,\n",
    "                        \"num_leaves\": self.num_leaves,\n",
    "                        \"learning_rate\": self.tree_lr,\n",
    "                        \"num_iterations\": self.tree_iter,\n",
    "                        \"force_col_wise\": True,\n",
    "                        \"data_sample_strategy\": \"goss\",\n",
    "                        \"path_smooth\": self.psmooth,\n",
    "                        \"random_seed\": 7,\n",
    "                        \"verbose\": 1\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.to(self.device)\n",
    "        if self.normalize:\n",
    "            seq_last = X[:, -1:, :]\n",
    "            X = X - seq_last\n",
    "\n",
    "        output = torch.zeros([X.size(0), self.pred_len, X.size(2)], dtype=X.dtype).to(X.device)\n",
    "        with torch.no_grad():\n",
    "            if self.use_RevIN:\n",
    "                l_in = self.revin(X, 'norm')\n",
    "\n",
    "            lin_in = X if not self.use_RevIN else l_in\n",
    "            if self.individual:\n",
    "                for i in range(self.channels):\n",
    "                    output[:, :, i] = self.Linear[i](lin_in[:, :, i].float())\n",
    "            else:\n",
    "                output = self.Linear(lin_in.permute(0, 2, 1).float()).permute(0, 2, 1)\n",
    "\n",
    "            if self.use_RevIN:\n",
    "                output = self.revin(output, 'denorm')\n",
    "\n",
    "        if self.treelb > 0:\n",
    "            if self.lb_data == '0':\n",
    "                if self.normalize:\n",
    "                    X += seq_last\n",
    "                X = torch.cat((X[:, -self.treelb:, :], output), dim=1)\n",
    "            elif self.lb_data == 'N':\n",
    "                X = torch.cat((X[:, -self.treelb:, :], output), dim=1)\n",
    "            else:\n",
    "                X = torch.cat((lin_in[:, -self.treelb:, :], output), dim=1)\n",
    "        else:\n",
    "            X = output\n",
    "\n",
    "        X, output = X.cpu().detach().numpy(), output.cpu().detach().numpy()\n",
    "\n",
    "        output2 = torch.zeros([output.shape[0], self.pred_len, output.shape[2]], dtype=output.dtype).to(output.device)\n",
    "        for i in range(self.channels):\n",
    "            dtest = X[:, :, i]\n",
    "            output2[:, :, i] = torch.tensor(self.Tree[i].predict(dtest, num_threads=10), dtype=torch.double)\n",
    "\n",
    "        if self.normalize:\n",
    "            seq_last = seq_last.cpu()\n",
    "            output = output + seq_last\n",
    "\n",
    "        return output + output2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59e297-ca37-45fa-9ae3-79d23101a9ef",
   "metadata": {},
   "source": [
    "# Define Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c49ac7d-77a5-4992-884d-ca8d0f3543c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset\n",
    "class Dataset_Custom(Dataset):\n",
    "    def __init__(self, flag='train', data_path=DS_NAME, scale=True, train_only=False):\n",
    "        # size [seq_len, label_len, pred_len]\n",
    "        self.seq_len = SEQ_LEN\n",
    "        self.label_len = LABEL_LEN\n",
    "        self.pred_len = PRED_LEN\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.scale = scale\n",
    "        self.train_only = train_only\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler_x = StandardScaler()\n",
    "        self.scaler_y = StandardScaler()\n",
    "        df_raw = pd.read_csv(self.data_path)\n",
    "\n",
    "        cols = list(df_raw.columns)\n",
    "        cols.remove('timestamp')\n",
    "        num_train = int(len(df_raw) * (0.7 if not self.train_only else 1))\n",
    "        num_test = int(len(df_raw) * 0.2)\n",
    "        num_vali = len(df_raw) - num_train - num_test\n",
    "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
    "        border2s = [num_train, num_train + num_vali, len(df_raw)]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        df_raw = df_raw[['timestamp'] + cols]\n",
    "        cols_data = df_raw.columns[1:]\n",
    "        df_data = df_raw[cols_data]\n",
    "\n",
    "        if self.scale:\n",
    "            train_data_x = df_data[border1s[0]:border2s[0]].drop(columns=['close'])\n",
    "            train_data_y = df_raw['close'][border1s[0]:border2s[0]].values.reshape(-1, 1)\n",
    "            self.scaler_x.fit(train_data_x.values)\n",
    "            self.scaler_y.fit(train_data_y)\n",
    "            data_x = self.scaler_x.transform(df_data.drop(columns=['close']).values)\n",
    "            data_y = self.scaler_y.transform(df_raw['close'].values.reshape(-1, 1)).flatten()\n",
    "        else:\n",
    "            data_x = df_data.drop(columns=['close']).values\n",
    "            data_y = df_raw['close'].values\n",
    "\n",
    "        self.data_x = data_x[border1:border2]\n",
    "        self.data_y = data_y[border1:border2]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler_y.inverse_transform(data)\n",
    "\n",
    "def validate(model, vali_loader, criterion):\n",
    "        errs = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in vali_loader:\n",
    "                batch_x = batch_x.float().to(DEVICE)\n",
    "                batch_y = batch_y[:, -PRED_LEN:,:].float().to(DEVICE)\n",
    "                outputs = model(batch_x)\n",
    "                outputs = outputs[:, -PRED_LEN:,:]\n",
    "                pred = outputs.detach().cpu()\n",
    "                true = batch_y.detach().cpu()\n",
    "                loss = criterion(pred, true)\n",
    "                errs.extend(np.abs(pred-true))\n",
    "\n",
    "        return np.mean(np.array(errs)**2), np.mean(errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49166293-f541-4506-9ed5-5c5b5028b2c5",
   "metadata": {},
   "source": [
    "# Load Model and Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f658662a-2e12-46bf-b0d0-4c92a8671c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltboost = Model()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    Dataset_Custom(flag='train'),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    drop_last=True)\n",
    "\n",
    "vali_loader = DataLoader(\n",
    "    Dataset_Custom(flag='val'),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    drop_last=True)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    Dataset_Custom(flag='test'),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d634b2c-cee4-41eb-9abc-532c6a2b1796",
   "metadata": {},
   "source": [
    "# Train linear part and evaluate it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c41c994e-cc7e-4e83-8302-29d32f2a73e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Training of linear part starting... =======\n",
      "\titers: 100, epoch: 1 | loss: 0.9106734\n",
      "\tspeed: 0.0024s/iter; left time: 97.2570s\n",
      "\titers: 200, epoch: 1 | loss: 0.7839079\n",
      "\tspeed: 0.0017s/iter; left time: 66.6413s\n",
      "\titers: 300, epoch: 1 | loss: 0.8969661\n",
      "\tspeed: 0.0016s/iter; left time: 66.2536s\n",
      "\titers: 400, epoch: 1 | loss: 0.8544950\n",
      "\tspeed: 0.0016s/iter; left time: 65.8978s\n",
      "\titers: 500, epoch: 1 | loss: 0.7279283\n",
      "\tspeed: 0.0016s/iter; left time: 65.6003s\n",
      "\titers: 600, epoch: 1 | loss: 0.8220646\n",
      "\tspeed: 0.0017s/iter; left time: 65.8626s\n",
      "\titers: 700, epoch: 1 | loss: 0.8170417\n",
      "\tspeed: 0.0016s/iter; left time: 65.2139s\n",
      "\titers: 800, epoch: 1 | loss: 0.8165528\n",
      "\tspeed: 0.0016s/iter; left time: 64.9314s\n",
      "\titers: 900, epoch: 1 | loss: 0.7678052\n",
      "\tspeed: 0.0015s/iter; left time: 58.4804s\n",
      "\titers: 1000, epoch: 1 | loss: 0.8241064\n",
      "\tspeed: 0.0015s/iter; left time: 57.9184s\n",
      "\titers: 1100, epoch: 1 | loss: 0.7644503\n",
      "\tspeed: 0.0010s/iter; left time: 39.5572s\n",
      "\titers: 1200, epoch: 1 | loss: 0.9836198\n",
      "\tspeed: 0.0016s/iter; left time: 63.8707s\n",
      "\titers: 1300, epoch: 1 | loss: 0.8065630\n",
      "\tspeed: 0.0016s/iter; left time: 63.9429s\n",
      "\titers: 1400, epoch: 1 | loss: 0.7508687\n",
      "\tspeed: 0.0016s/iter; left time: 63.0564s\n",
      "\titers: 1500, epoch: 1 | loss: 0.9397963\n",
      "\tspeed: 0.0015s/iter; left time: 58.4040s\n",
      "\titers: 1600, epoch: 1 | loss: 0.7884577\n",
      "\tspeed: 0.0016s/iter; left time: 64.1495s\n",
      "\titers: 1700, epoch: 1 | loss: 0.6356189\n",
      "\tspeed: 0.0016s/iter; left time: 63.9041s\n",
      "\titers: 1800, epoch: 1 | loss: 0.8158649\n",
      "\tspeed: 0.0016s/iter; left time: 63.1101s\n",
      "\titers: 1900, epoch: 1 | loss: 0.8158438\n",
      "\tspeed: 0.0016s/iter; left time: 62.9880s\n",
      "\titers: 2000, epoch: 1 | loss: 0.8599441\n",
      "\tspeed: 0.0016s/iter; left time: 62.4962s\n",
      "\titers: 2100, epoch: 1 | loss: 0.9945654\n",
      "\tspeed: 0.0014s/iter; left time: 52.0719s\n",
      "\titers: 2200, epoch: 1 | loss: 1.0571225\n",
      "\tspeed: 0.0009s/iter; left time: 33.4792s\n",
      "\titers: 2300, epoch: 1 | loss: 0.8817901\n",
      "\tspeed: 0.0009s/iter; left time: 33.3589s\n",
      "\titers: 2400, epoch: 1 | loss: 0.7902717\n",
      "\tspeed: 0.0009s/iter; left time: 33.4574s\n",
      "\titers: 2500, epoch: 1 | loss: 0.8503651\n",
      "\tspeed: 0.0009s/iter; left time: 34.7665s\n",
      "\titers: 2600, epoch: 1 | loss: 0.6885952\n",
      "\tspeed: 0.0010s/iter; left time: 36.0806s\n",
      "\titers: 2700, epoch: 1 | loss: 0.7657301\n",
      "\tspeed: 0.0010s/iter; left time: 38.8269s\n",
      "\titers: 2800, epoch: 1 | loss: 0.7020000\n",
      "\tspeed: 0.0009s/iter; left time: 34.7942s\n",
      "\titers: 2900, epoch: 1 | loss: 0.8103708\n",
      "\tspeed: 0.0009s/iter; left time: 33.0313s\n",
      "\titers: 3000, epoch: 1 | loss: 0.7637988\n",
      "\tspeed: 0.0009s/iter; left time: 32.6760s\n",
      "\titers: 3100, epoch: 1 | loss: 0.9426010\n",
      "\tspeed: 0.0009s/iter; left time: 32.5565s\n",
      "\titers: 3200, epoch: 1 | loss: 0.8268576\n",
      "\tspeed: 0.0010s/iter; left time: 35.7500s\n",
      "\titers: 3300, epoch: 1 | loss: 0.7228016\n",
      "\tspeed: 0.0010s/iter; left time: 37.6092s\n",
      "\titers: 3400, epoch: 1 | loss: 0.8817891\n",
      "\tspeed: 0.0010s/iter; left time: 36.0058s\n",
      "\titers: 3500, epoch: 1 | loss: 0.9467888\n",
      "\tspeed: 0.0009s/iter; left time: 33.8857s\n",
      "\titers: 3600, epoch: 1 | loss: 0.8120496\n",
      "\tspeed: 0.0009s/iter; left time: 32.7972s\n",
      "\titers: 3700, epoch: 1 | loss: 0.7964929\n",
      "\tspeed: 0.0009s/iter; left time: 34.3102s\n",
      "\titers: 3800, epoch: 1 | loss: 0.7757162\n",
      "\tspeed: 0.0013s/iter; left time: 46.3369s\n",
      "\titers: 3900, epoch: 1 | loss: 0.7713095\n",
      "\tspeed: 0.0017s/iter; left time: 60.4147s\n",
      "\titers: 4000, epoch: 1 | loss: 0.9508554\n",
      "\tspeed: 0.0016s/iter; left time: 60.1297s\n",
      "\titers: 4100, epoch: 1 | loss: 0.7242891\n",
      "\tspeed: 0.0016s/iter; left time: 59.7037s\n",
      "\titers: 4200, epoch: 1 | loss: 0.6885122\n",
      "\tspeed: 0.0014s/iter; left time: 49.6631s\n",
      "\titers: 4300, epoch: 1 | loss: 0.8283142\n",
      "\tspeed: 0.0015s/iter; left time: 53.9501s\n",
      "\titers: 4400, epoch: 1 | loss: 0.8685657\n",
      "\tspeed: 0.0016s/iter; left time: 59.4111s\n",
      "\titers: 4500, epoch: 1 | loss: 0.9846789\n",
      "\tspeed: 0.0014s/iter; left time: 50.2857s\n",
      "\titers: 4600, epoch: 1 | loss: 0.9424260\n",
      "\tspeed: 0.0009s/iter; left time: 33.6393s\n",
      "\titers: 4700, epoch: 1 | loss: 0.9460183\n",
      "\tspeed: 0.0012s/iter; left time: 41.3842s\n",
      "\titers: 4800, epoch: 1 | loss: 0.5953599\n",
      "\tspeed: 0.0010s/iter; left time: 35.4950s\n",
      "\titers: 4900, epoch: 1 | loss: 0.7262537\n",
      "\tspeed: 0.0015s/iter; left time: 54.7186s\n",
      "\titers: 5000, epoch: 1 | loss: 0.9084026\n",
      "\tspeed: 0.0016s/iter; left time: 57.1378s\n",
      "\titers: 5100, epoch: 1 | loss: 0.8015156\n",
      "\tspeed: 0.0015s/iter; left time: 53.2929s\n",
      "\titers: 5200, epoch: 1 | loss: 0.8351684\n",
      "\tspeed: 0.0015s/iter; left time: 52.7945s\n",
      "\titers: 5300, epoch: 1 | loss: 0.6935316\n",
      "\tspeed: 0.0013s/iter; left time: 44.4073s\n",
      "\titers: 5400, epoch: 1 | loss: 0.6295881\n",
      "\tspeed: 0.0009s/iter; left time: 33.0315s\n",
      "\titers: 5500, epoch: 1 | loss: 0.7600800\n",
      "\tspeed: 0.0009s/iter; left time: 32.9707s\n",
      "\titers: 5600, epoch: 1 | loss: 0.9233712\n",
      "\tspeed: 0.0009s/iter; left time: 32.9450s\n",
      "\titers: 5700, epoch: 1 | loss: 0.8154179\n",
      "\tspeed: 0.0010s/iter; left time: 36.2181s\n",
      "\titers: 5800, epoch: 1 | loss: 0.7903768\n",
      "\tspeed: 0.0010s/iter; left time: 35.8147s\n",
      "\titers: 5900, epoch: 1 | loss: 0.8971165\n",
      "\tspeed: 0.0012s/iter; left time: 42.8818s\n",
      "\titers: 6000, epoch: 1 | loss: 0.8200041\n",
      "\tspeed: 0.0015s/iter; left time: 50.0940s\n",
      "\titers: 6100, epoch: 1 | loss: 0.9455023\n",
      "\tspeed: 0.0016s/iter; left time: 54.9759s\n",
      "\titers: 6200, epoch: 1 | loss: 0.9720022\n",
      "\tspeed: 0.0013s/iter; left time: 43.6478s\n",
      "\titers: 6300, epoch: 1 | loss: 0.8522928\n",
      "\tspeed: 0.0016s/iter; left time: 56.1467s\n",
      "\titers: 6400, epoch: 1 | loss: 0.6857809\n",
      "\tspeed: 0.0016s/iter; left time: 55.6053s\n",
      "\titers: 6500, epoch: 1 | loss: 0.9741982\n",
      "\tspeed: 0.0015s/iter; left time: 50.8152s\n",
      "\titers: 6600, epoch: 1 | loss: 0.8388748\n",
      "\tspeed: 0.0014s/iter; left time: 48.8849s\n",
      "\titers: 6700, epoch: 1 | loss: 0.7625105\n",
      "\tspeed: 0.0015s/iter; left time: 50.0561s\n",
      "\titers: 6800, epoch: 1 | loss: 1.0401508\n",
      "\tspeed: 0.0016s/iter; left time: 52.8054s\n",
      "\titers: 6900, epoch: 1 | loss: 0.7595432\n",
      "\tspeed: 0.0015s/iter; left time: 50.4923s\n",
      "\titers: 7000, epoch: 1 | loss: 0.8896111\n",
      "\tspeed: 0.0010s/iter; left time: 32.3703s\n",
      "\titers: 7100, epoch: 1 | loss: 0.8368007\n",
      "\tspeed: 0.0010s/iter; left time: 33.4472s\n",
      "\titers: 7200, epoch: 1 | loss: 0.9434369\n",
      "\tspeed: 0.0009s/iter; left time: 30.5163s\n",
      "\titers: 7300, epoch: 1 | loss: 1.0855472\n",
      "\tspeed: 0.0009s/iter; left time: 29.4599s\n",
      "\titers: 7400, epoch: 1 | loss: 0.8672690\n",
      "\tspeed: 0.0014s/iter; left time: 47.3702s\n",
      "\titers: 7500, epoch: 1 | loss: 0.8044409\n",
      "\tspeed: 0.0015s/iter; left time: 49.0328s\n",
      "\titers: 7600, epoch: 1 | loss: 0.8224546\n",
      "\tspeed: 0.0014s/iter; left time: 46.3472s\n",
      "\titers: 7700, epoch: 1 | loss: 0.8956496\n",
      "\tspeed: 0.0010s/iter; left time: 33.1712s\n",
      "\titers: 7800, epoch: 1 | loss: 0.8284043\n",
      "\tspeed: 0.0010s/iter; left time: 32.7177s\n",
      "\titers: 7900, epoch: 1 | loss: 0.9804574\n",
      "\tspeed: 0.0016s/iter; left time: 53.7437s\n",
      "\titers: 8000, epoch: 1 | loss: 0.6891046\n",
      "\tspeed: 0.0016s/iter; left time: 53.5418s\n",
      "\titers: 8100, epoch: 1 | loss: 0.8646390\n",
      "\tspeed: 0.0016s/iter; left time: 53.2172s\n",
      "\titers: 8200, epoch: 1 | loss: 0.7666614\n",
      "\tspeed: 0.0016s/iter; left time: 53.0414s\n",
      "\titers: 8300, epoch: 1 | loss: 0.8363043\n",
      "\tspeed: 0.0013s/iter; left time: 40.3770s\n",
      "\titers: 8400, epoch: 1 | loss: 0.7860007\n",
      "\tspeed: 0.0016s/iter; left time: 49.7446s\n",
      "\titers: 8500, epoch: 1 | loss: 0.8600324\n",
      "\tspeed: 0.0016s/iter; left time: 50.3179s\n",
      "\titers: 8600, epoch: 1 | loss: 0.6763021\n",
      "\tspeed: 0.0014s/iter; left time: 44.7652s\n",
      "\titers: 8700, epoch: 1 | loss: 0.8032574\n",
      "\tspeed: 0.0013s/iter; left time: 42.3485s\n",
      "\titers: 8800, epoch: 1 | loss: 0.7996319\n",
      "\tspeed: 0.0017s/iter; left time: 52.3080s\n",
      "\titers: 8900, epoch: 1 | loss: 0.9113070\n",
      "\tspeed: 0.0014s/iter; left time: 45.5515s\n",
      "\titers: 9000, epoch: 1 | loss: 0.9538193\n",
      "\tspeed: 0.0016s/iter; left time: 51.5620s\n",
      "\titers: 9100, epoch: 1 | loss: 0.6548569\n",
      "\tspeed: 0.0014s/iter; left time: 44.4332s\n",
      "\titers: 9200, epoch: 1 | loss: 0.7678778\n",
      "\tspeed: 0.0009s/iter; left time: 29.0804s\n",
      "\titers: 9300, epoch: 1 | loss: 0.6208081\n",
      "\tspeed: 0.0009s/iter; left time: 28.2591s\n",
      "\titers: 9400, epoch: 1 | loss: 0.8134896\n",
      "\tspeed: 0.0009s/iter; left time: 27.7864s\n",
      "\titers: 9500, epoch: 1 | loss: 0.7342097\n",
      "\tspeed: 0.0009s/iter; left time: 27.6986s\n",
      "\titers: 9600, epoch: 1 | loss: 0.8507389\n",
      "\tspeed: 0.0009s/iter; left time: 27.5081s\n",
      "\titers: 9700, epoch: 1 | loss: 0.7832462\n",
      "\tspeed: 0.0009s/iter; left time: 27.5128s\n",
      "\titers: 9800, epoch: 1 | loss: 0.7163938\n",
      "\tspeed: 0.0009s/iter; left time: 27.5143s\n",
      "\titers: 9900, epoch: 1 | loss: 0.8448107\n",
      "\tspeed: 0.0009s/iter; left time: 27.4534s\n",
      "\titers: 10000, epoch: 1 | loss: 0.9752974\n",
      "\tspeed: 0.0011s/iter; left time: 33.4862s\n",
      "\titers: 10100, epoch: 1 | loss: 0.8672359\n",
      "\tspeed: 0.0009s/iter; left time: 27.7504s\n",
      "\titers: 10200, epoch: 1 | loss: 0.9101240\n",
      "\tspeed: 0.0009s/iter; left time: 27.0456s\n",
      "\titers: 10300, epoch: 1 | loss: 0.8127318\n",
      "\tspeed: 0.0009s/iter; left time: 26.6048s\n",
      "\titers: 10400, epoch: 1 | loss: 0.7722589\n",
      "\tspeed: 0.0009s/iter; left time: 28.1139s\n",
      "\titers: 10500, epoch: 1 | loss: 0.7963852\n",
      "\tspeed: 0.0011s/iter; left time: 33.2641s\n",
      "\titers: 10600, epoch: 1 | loss: 0.8671407\n",
      "\tspeed: 0.0013s/iter; left time: 39.2962s\n",
      "\titers: 10700, epoch: 1 | loss: 0.9480693\n",
      "\tspeed: 0.0010s/iter; left time: 30.5625s\n",
      "\titers: 10800, epoch: 1 | loss: 0.5958810\n",
      "\tspeed: 0.0011s/iter; left time: 31.8935s\n",
      "\titers: 10900, epoch: 1 | loss: 0.8149311\n",
      "\tspeed: 0.0015s/iter; left time: 44.3003s\n",
      "\titers: 11000, epoch: 1 | loss: 0.7706689\n",
      "\tspeed: 0.0016s/iter; left time: 48.3886s\n",
      "\titers: 11100, epoch: 1 | loss: 0.6358885\n",
      "\tspeed: 0.0015s/iter; left time: 42.6428s\n",
      "\titers: 11200, epoch: 1 | loss: 0.6964247\n",
      "\tspeed: 0.0015s/iter; left time: 43.0222s\n",
      "\titers: 11300, epoch: 1 | loss: 0.8796874\n",
      "\tspeed: 0.0016s/iter; left time: 46.0740s\n",
      "\titers: 11400, epoch: 1 | loss: 0.7606299\n",
      "\tspeed: 0.0012s/iter; left time: 36.1440s\n",
      "\titers: 11500, epoch: 1 | loss: 0.7708661\n",
      "\tspeed: 0.0016s/iter; left time: 45.3767s\n",
      "\titers: 11600, epoch: 1 | loss: 0.6879572\n",
      "\tspeed: 0.0012s/iter; left time: 34.4576s\n",
      "\titers: 11700, epoch: 1 | loss: 0.8178786\n",
      "\tspeed: 0.0009s/iter; left time: 25.8681s\n",
      "\titers: 11800, epoch: 1 | loss: 0.7607772\n",
      "\tspeed: 0.0009s/iter; left time: 25.5366s\n",
      "\titers: 11900, epoch: 1 | loss: 0.9528159\n",
      "\tspeed: 0.0009s/iter; left time: 25.6492s\n",
      "\titers: 12000, epoch: 1 | loss: 0.7890519\n",
      "\tspeed: 0.0009s/iter; left time: 25.1837s\n",
      "\titers: 12100, epoch: 1 | loss: 0.8108739\n",
      "\tspeed: 0.0009s/iter; left time: 25.2109s\n",
      "\titers: 12200, epoch: 1 | loss: 0.8226699\n",
      "\tspeed: 0.0009s/iter; left time: 25.2971s\n",
      "\titers: 12300, epoch: 1 | loss: 0.8507999\n",
      "\tspeed: 0.0014s/iter; left time: 39.9585s\n",
      "\titers: 12400, epoch: 1 | loss: 0.7675980\n",
      "\tspeed: 0.0010s/iter; left time: 28.8207s\n",
      "\titers: 12500, epoch: 1 | loss: 0.8597597\n",
      "\tspeed: 0.0010s/iter; left time: 26.7085s\n",
      "\titers: 12600, epoch: 1 | loss: 1.1193978\n",
      "\tspeed: 0.0010s/iter; left time: 28.6913s\n",
      "\titers: 12700, epoch: 1 | loss: 0.6972421\n",
      "\tspeed: 0.0015s/iter; left time: 40.5999s\n",
      "\titers: 12800, epoch: 1 | loss: 0.9457420\n",
      "\tspeed: 0.0016s/iter; left time: 45.6404s\n",
      "\titers: 12900, epoch: 1 | loss: 0.7886260\n",
      "\tspeed: 0.0015s/iter; left time: 41.3954s\n",
      "\titers: 13000, epoch: 1 | loss: 0.8603576\n",
      "\tspeed: 0.0016s/iter; left time: 44.1642s\n",
      "\titers: 13100, epoch: 1 | loss: 0.8598239\n",
      "\tspeed: 0.0014s/iter; left time: 37.2425s\n",
      "\titers: 13200, epoch: 1 | loss: 0.7636592\n",
      "\tspeed: 0.0016s/iter; left time: 44.8371s\n",
      "\titers: 13300, epoch: 1 | loss: 0.9431933\n",
      "\tspeed: 0.0016s/iter; left time: 44.6812s\n",
      "\titers: 13400, epoch: 1 | loss: 0.7286530\n",
      "\tspeed: 0.0016s/iter; left time: 44.6127s\n",
      "\titers: 13500, epoch: 1 | loss: 0.8389910\n",
      "\tspeed: 0.0013s/iter; left time: 34.1786s\n",
      "\titers: 13600, epoch: 1 | loss: 0.8450865\n",
      "\tspeed: 0.0009s/iter; left time: 24.9926s\n",
      "\titers: 13700, epoch: 1 | loss: 0.8080846\n",
      "\tspeed: 0.0009s/iter; left time: 23.6031s\n",
      "\titers: 13800, epoch: 1 | loss: 0.7590185\n",
      "\tspeed: 0.0009s/iter; left time: 23.6202s\n",
      "\titers: 13900, epoch: 1 | loss: 0.8026547\n",
      "\tspeed: 0.0010s/iter; left time: 26.8678s\n",
      "\titers: 14000, epoch: 1 | loss: 0.8612970\n",
      "\tspeed: 0.0017s/iter; left time: 43.9699s\n",
      "\titers: 14100, epoch: 1 | loss: 0.8126796\n",
      "\tspeed: 0.0015s/iter; left time: 39.1105s\n",
      "\titers: 14200, epoch: 1 | loss: 0.8452107\n",
      "\tspeed: 0.0016s/iter; left time: 43.2907s\n",
      "\titers: 14300, epoch: 1 | loss: 0.7425264\n",
      "\tspeed: 0.0016s/iter; left time: 42.9907s\n",
      "\titers: 14400, epoch: 1 | loss: 0.6944306\n",
      "\tspeed: 0.0015s/iter; left time: 38.3795s\n",
      "\titers: 14500, epoch: 1 | loss: 0.7849912\n",
      "\tspeed: 0.0015s/iter; left time: 37.6933s\n",
      "\titers: 14600, epoch: 1 | loss: 0.8282620\n",
      "\tspeed: 0.0010s/iter; left time: 26.9007s\n",
      "\titers: 14700, epoch: 1 | loss: 0.8905069\n",
      "\tspeed: 0.0010s/iter; left time: 26.7397s\n",
      "\titers: 14800, epoch: 1 | loss: 0.8677078\n",
      "\tspeed: 0.0014s/iter; left time: 35.9842s\n",
      "\titers: 14900, epoch: 1 | loss: 0.7776056\n",
      "\tspeed: 0.0013s/iter; left time: 33.4851s\n",
      "\titers: 15000, epoch: 1 | loss: 0.9501101\n",
      "\tspeed: 0.0016s/iter; left time: 40.2308s\n",
      "\titers: 15100, epoch: 1 | loss: 0.8195081\n",
      "\tspeed: 0.0010s/iter; left time: 25.4142s\n",
      "\titers: 15200, epoch: 1 | loss: 0.8489271\n",
      "\tspeed: 0.0013s/iter; left time: 32.0256s\n",
      "\titers: 15300, epoch: 1 | loss: 0.7558412\n",
      "\tspeed: 0.0016s/iter; left time: 41.5501s\n",
      "\titers: 15400, epoch: 1 | loss: 0.7168418\n",
      "\tspeed: 0.0010s/iter; left time: 25.2908s\n",
      "\titers: 15500, epoch: 1 | loss: 0.8185486\n",
      "\tspeed: 0.0009s/iter; left time: 22.4215s\n",
      "\titers: 15600, epoch: 1 | loss: 0.7208014\n",
      "\tspeed: 0.0010s/iter; left time: 25.5590s\n",
      "\titers: 15700, epoch: 1 | loss: 0.5451793\n",
      "\tspeed: 0.0011s/iter; left time: 26.1870s\n",
      "\titers: 15800, epoch: 1 | loss: 0.8625336\n",
      "\tspeed: 0.0012s/iter; left time: 28.8326s\n",
      "\titers: 15900, epoch: 1 | loss: 0.7066410\n",
      "\tspeed: 0.0015s/iter; left time: 37.2697s\n",
      "\titers: 16000, epoch: 1 | loss: 0.8157958\n",
      "\tspeed: 0.0016s/iter; left time: 40.3617s\n",
      "\titers: 16100, epoch: 1 | loss: 0.8445780\n",
      "\tspeed: 0.0016s/iter; left time: 38.6970s\n",
      "\titers: 16200, epoch: 1 | loss: 0.7657863\n",
      "\tspeed: 0.0016s/iter; left time: 39.7776s\n",
      "\titers: 16300, epoch: 1 | loss: 1.0220814\n",
      "\tspeed: 0.0010s/iter; left time: 23.7615s\n",
      "\titers: 16400, epoch: 1 | loss: 0.8618809\n",
      "\tspeed: 0.0010s/iter; left time: 24.4079s\n",
      "\titers: 16500, epoch: 1 | loss: 0.8761809\n",
      "\tspeed: 0.0010s/iter; left time: 23.4089s\n",
      "\titers: 16600, epoch: 1 | loss: 0.8683807\n",
      "\tspeed: 0.0010s/iter; left time: 24.8709s\n",
      "\titers: 16700, epoch: 1 | loss: 0.8903614\n",
      "\tspeed: 0.0009s/iter; left time: 21.2979s\n",
      "\titers: 16800, epoch: 1 | loss: 0.9440499\n",
      "\tspeed: 0.0009s/iter; left time: 21.0890s\n",
      "\titers: 16900, epoch: 1 | loss: 0.7904978\n",
      "\tspeed: 0.0009s/iter; left time: 21.1951s\n",
      "\titers: 17000, epoch: 1 | loss: 1.0490617\n",
      "\tspeed: 0.0014s/iter; left time: 33.8472s\n",
      "\titers: 17100, epoch: 1 | loss: 0.8647477\n",
      "\tspeed: 0.0011s/iter; left time: 26.1724s\n",
      "\titers: 17200, epoch: 1 | loss: 0.8029697\n",
      "\tspeed: 0.0009s/iter; left time: 21.3296s\n",
      "\titers: 17300, epoch: 1 | loss: 0.7959276\n",
      "\tspeed: 0.0010s/iter; left time: 22.3791s\n",
      "\titers: 17400, epoch: 1 | loss: 0.9135945\n",
      "\tspeed: 0.0012s/iter; left time: 27.1742s\n",
      "\titers: 17500, epoch: 1 | loss: 0.8432519\n",
      "\tspeed: 0.0009s/iter; left time: 20.2259s\n",
      "\titers: 17600, epoch: 1 | loss: 0.8544242\n",
      "\tspeed: 0.0009s/iter; left time: 20.1839s\n",
      "\titers: 17700, epoch: 1 | loss: 0.8433308\n",
      "\tspeed: 0.0009s/iter; left time: 20.2215s\n",
      "\titers: 17800, epoch: 1 | loss: 0.7373897\n",
      "\tspeed: 0.0010s/iter; left time: 22.8113s\n",
      "\titers: 17900, epoch: 1 | loss: 0.6922334\n",
      "\tspeed: 0.0009s/iter; left time: 21.1253s\n",
      "\titers: 18000, epoch: 1 | loss: 0.7835245\n",
      "\tspeed: 0.0012s/iter; left time: 26.3096s\n",
      "\titers: 18100, epoch: 1 | loss: 0.8625473\n",
      "\tspeed: 0.0012s/iter; left time: 26.4512s\n",
      "\titers: 18200, epoch: 1 | loss: 0.9052295\n",
      "\tspeed: 0.0009s/iter; left time: 20.0050s\n",
      "\titers: 18300, epoch: 1 | loss: 0.9195050\n",
      "\tspeed: 0.0009s/iter; left time: 21.0173s\n",
      "\titers: 18400, epoch: 1 | loss: 0.8949783\n",
      "\tspeed: 0.0016s/iter; left time: 35.8749s\n",
      "\titers: 18500, epoch: 1 | loss: 0.7408195\n",
      "\tspeed: 0.0013s/iter; left time: 28.5235s\n",
      "\titers: 18600, epoch: 1 | loss: 0.8554646\n",
      "\tspeed: 0.0013s/iter; left time: 29.0767s\n",
      "\titers: 18700, epoch: 1 | loss: 0.8931871\n",
      "\tspeed: 0.0014s/iter; left time: 31.5584s\n",
      "\titers: 18800, epoch: 1 | loss: 1.0084912\n",
      "\tspeed: 0.0012s/iter; left time: 24.9438s\n",
      "\titers: 18900, epoch: 1 | loss: 0.9853504\n",
      "\tspeed: 0.0009s/iter; left time: 20.1075s\n",
      "\titers: 19000, epoch: 1 | loss: 0.8606179\n",
      "\tspeed: 0.0012s/iter; left time: 26.1850s\n",
      "\titers: 19100, epoch: 1 | loss: 1.0707694\n",
      "\tspeed: 0.0011s/iter; left time: 23.3862s\n",
      "\titers: 19200, epoch: 1 | loss: 0.9679398\n",
      "\tspeed: 0.0009s/iter; left time: 19.2071s\n",
      "\titers: 19300, epoch: 1 | loss: 0.7916685\n",
      "\tspeed: 0.0010s/iter; left time: 20.2991s\n",
      "\titers: 19400, epoch: 1 | loss: 0.7715113\n",
      "\tspeed: 0.0010s/iter; left time: 21.5054s\n",
      "\titers: 19500, epoch: 1 | loss: 0.8378693\n",
      "\tspeed: 0.0009s/iter; left time: 19.5923s\n",
      "\titers: 19600, epoch: 1 | loss: 0.6362792\n",
      "\tspeed: 0.0009s/iter; left time: 18.6738s\n",
      "\titers: 19700, epoch: 1 | loss: 0.7560913\n",
      "\tspeed: 0.0009s/iter; left time: 19.3413s\n",
      "\titers: 19800, epoch: 1 | loss: 0.7112066\n",
      "\tspeed: 0.0009s/iter; left time: 18.3750s\n",
      "\titers: 19900, epoch: 1 | loss: 0.6574459\n",
      "\tspeed: 0.0009s/iter; left time: 18.3871s\n",
      "\titers: 20000, epoch: 1 | loss: 0.9472409\n",
      "\tspeed: 0.0009s/iter; left time: 18.5718s\n",
      "\titers: 20100, epoch: 1 | loss: 0.7845885\n",
      "\tspeed: 0.0016s/iter; left time: 32.1597s\n",
      "\titers: 20200, epoch: 1 | loss: 0.9263728\n",
      "\tspeed: 0.0011s/iter; left time: 21.6715s\n",
      "\titers: 20300, epoch: 1 | loss: 0.7473599\n",
      "\tspeed: 0.0009s/iter; left time: 17.8662s\n",
      "\titers: 20400, epoch: 1 | loss: 0.5285972\n",
      "\tspeed: 0.0009s/iter; left time: 17.9233s\n",
      "\titers: 20500, epoch: 1 | loss: 0.9515656\n",
      "\tspeed: 0.0011s/iter; left time: 22.6387s\n",
      "\titers: 20600, epoch: 1 | loss: 0.8127692\n",
      "\tspeed: 0.0016s/iter; left time: 32.7706s\n",
      "\titers: 20700, epoch: 1 | loss: 0.9575115\n",
      "\tspeed: 0.0012s/iter; left time: 23.0635s\n",
      "\titers: 20800, epoch: 1 | loss: 0.7346445\n",
      "\tspeed: 0.0009s/iter; left time: 18.6267s\n",
      "\titers: 20900, epoch: 1 | loss: 0.6091144\n",
      "\tspeed: 0.0016s/iter; left time: 31.3415s\n",
      "\titers: 21000, epoch: 1 | loss: 0.8912680\n",
      "\tspeed: 0.0014s/iter; left time: 27.8243s\n",
      "\titers: 21100, epoch: 1 | loss: 0.6324263\n",
      "\tspeed: 0.0015s/iter; left time: 28.3133s\n",
      "\titers: 21200, epoch: 1 | loss: 0.7102085\n",
      "\tspeed: 0.0016s/iter; left time: 31.6832s\n",
      "\titers: 21300, epoch: 1 | loss: 0.7612408\n",
      "\tspeed: 0.0016s/iter; left time: 31.3961s\n",
      "\titers: 21400, epoch: 1 | loss: 0.9208823\n",
      "\tspeed: 0.0016s/iter; left time: 31.2975s\n",
      "\titers: 21500, epoch: 1 | loss: 0.7965772\n",
      "\tspeed: 0.0016s/iter; left time: 31.0312s\n",
      "\titers: 21600, epoch: 1 | loss: 0.7884156\n",
      "\tspeed: 0.0011s/iter; left time: 21.1283s\n",
      "\titers: 21700, epoch: 1 | loss: 0.9446349\n",
      "\tspeed: 0.0014s/iter; left time: 26.3858s\n",
      "\titers: 21800, epoch: 1 | loss: 0.6650034\n",
      "\tspeed: 0.0013s/iter; left time: 23.7833s\n",
      "\titers: 21900, epoch: 1 | loss: 1.0156407\n",
      "\tspeed: 0.0016s/iter; left time: 29.3377s\n",
      "\titers: 22000, epoch: 1 | loss: 0.7847030\n",
      "\tspeed: 0.0017s/iter; left time: 30.5474s\n",
      "\titers: 22100, epoch: 1 | loss: 0.8791547\n",
      "\tspeed: 0.0016s/iter; left time: 28.5717s\n",
      "\titers: 22200, epoch: 1 | loss: 0.8798501\n",
      "\tspeed: 0.0016s/iter; left time: 29.7808s\n",
      "\titers: 22300, epoch: 1 | loss: 0.7377692\n",
      "\tspeed: 0.0011s/iter; left time: 19.5421s\n",
      "\titers: 22400, epoch: 1 | loss: 0.8318378\n",
      "\tspeed: 0.0013s/iter; left time: 23.5350s\n",
      "\titers: 22500, epoch: 1 | loss: 0.8114699\n",
      "\tspeed: 0.0015s/iter; left time: 27.2278s\n",
      "\titers: 22600, epoch: 1 | loss: 0.8199248\n",
      "\tspeed: 0.0014s/iter; left time: 25.1485s\n",
      "\titers: 22700, epoch: 1 | loss: 0.6622378\n",
      "\tspeed: 0.0017s/iter; left time: 29.3641s\n",
      "\titers: 22800, epoch: 1 | loss: 0.8654439\n",
      "\tspeed: 0.0014s/iter; left time: 25.4593s\n",
      "\titers: 22900, epoch: 1 | loss: 0.7156808\n",
      "\tspeed: 0.0010s/iter; left time: 18.2961s\n",
      "\titers: 23000, epoch: 1 | loss: 0.8161116\n",
      "\tspeed: 0.0010s/iter; left time: 18.2294s\n",
      "\titers: 23100, epoch: 1 | loss: 0.8700244\n",
      "\tspeed: 0.0009s/iter; left time: 16.4048s\n",
      "\titers: 23200, epoch: 1 | loss: 0.8384769\n",
      "\tspeed: 0.0009s/iter; left time: 15.1254s\n",
      "\titers: 23300, epoch: 1 | loss: 0.7475247\n",
      "\tspeed: 0.0009s/iter; left time: 15.1065s\n",
      "\titers: 23400, epoch: 1 | loss: 0.9040081\n",
      "\tspeed: 0.0009s/iter; left time: 14.9828s\n",
      "\titers: 23500, epoch: 1 | loss: 0.8002428\n",
      "\tspeed: 0.0009s/iter; left time: 14.9052s\n",
      "\titers: 23600, epoch: 1 | loss: 0.7500798\n",
      "\tspeed: 0.0009s/iter; left time: 14.9395s\n",
      "\titers: 23700, epoch: 1 | loss: 1.0283540\n",
      "\tspeed: 0.0009s/iter; left time: 14.7062s\n",
      "\titers: 23800, epoch: 1 | loss: 0.8989479\n",
      "\tspeed: 0.0010s/iter; left time: 16.0105s\n",
      "\titers: 23900, epoch: 1 | loss: 0.9254575\n",
      "\tspeed: 0.0014s/iter; left time: 24.0402s\n",
      "\titers: 24000, epoch: 1 | loss: 0.7587820\n",
      "\tspeed: 0.0012s/iter; left time: 20.3898s\n",
      "\titers: 24100, epoch: 1 | loss: 0.7120401\n",
      "\tspeed: 0.0009s/iter; left time: 15.2665s\n",
      "\titers: 24200, epoch: 1 | loss: 0.7005168\n",
      "\tspeed: 0.0009s/iter; left time: 14.1706s\n",
      "\titers: 24300, epoch: 1 | loss: 0.8735615\n",
      "\tspeed: 0.0009s/iter; left time: 14.1806s\n",
      "\titers: 24400, epoch: 1 | loss: 0.7916028\n",
      "\tspeed: 0.0009s/iter; left time: 14.8680s\n",
      "\titers: 24500, epoch: 1 | loss: 0.6943481\n",
      "\tspeed: 0.0010s/iter; left time: 15.8658s\n",
      "\titers: 24600, epoch: 1 | loss: 0.7699277\n",
      "\tspeed: 0.0010s/iter; left time: 15.3109s\n",
      "\titers: 24700, epoch: 1 | loss: 0.8620669\n",
      "\tspeed: 0.0012s/iter; left time: 19.0542s\n",
      "\titers: 24800, epoch: 1 | loss: 0.9680524\n",
      "\tspeed: 0.0009s/iter; left time: 13.9958s\n",
      "\titers: 24900, epoch: 1 | loss: 0.8650945\n",
      "\tspeed: 0.0009s/iter; left time: 14.0145s\n",
      "\titers: 25000, epoch: 1 | loss: 0.9276152\n",
      "\tspeed: 0.0009s/iter; left time: 13.8364s\n",
      "\titers: 25100, epoch: 1 | loss: 0.7989500\n",
      "\tspeed: 0.0009s/iter; left time: 14.0328s\n",
      "\titers: 25200, epoch: 1 | loss: 0.9603009\n",
      "\tspeed: 0.0012s/iter; left time: 18.4373s\n",
      "\titers: 25300, epoch: 1 | loss: 0.7820950\n",
      "\tspeed: 0.0009s/iter; left time: 14.2560s\n",
      "\titers: 25400, epoch: 1 | loss: 0.7242018\n",
      "\tspeed: 0.0009s/iter; left time: 13.4494s\n",
      "\titers: 25500, epoch: 1 | loss: 0.9832099\n",
      "\tspeed: 0.0009s/iter; left time: 13.3488s\n",
      "\titers: 25600, epoch: 1 | loss: 0.7124695\n",
      "\tspeed: 0.0009s/iter; left time: 13.2810s\n",
      "\titers: 25700, epoch: 1 | loss: 0.8775909\n",
      "\tspeed: 0.0009s/iter; left time: 13.1405s\n",
      "\titers: 25800, epoch: 1 | loss: 0.8713793\n",
      "\tspeed: 0.0009s/iter; left time: 13.1095s\n",
      "\titers: 25900, epoch: 1 | loss: 0.9747307\n",
      "\tspeed: 0.0009s/iter; left time: 12.9878s\n",
      "\titers: 26000, epoch: 1 | loss: 0.9970250\n",
      "\tspeed: 0.0009s/iter; left time: 12.8969s\n",
      "\titers: 26100, epoch: 1 | loss: 1.0552943\n",
      "\tspeed: 0.0009s/iter; left time: 12.8482s\n",
      "\titers: 26200, epoch: 1 | loss: 0.7674348\n",
      "\tspeed: 0.0009s/iter; left time: 12.7387s\n",
      "\titers: 26300, epoch: 1 | loss: 0.7439120\n",
      "\tspeed: 0.0014s/iter; left time: 19.1937s\n",
      "\titers: 26400, epoch: 1 | loss: 0.6279173\n",
      "\tspeed: 0.0016s/iter; left time: 23.1199s\n",
      "\titers: 26500, epoch: 1 | loss: 0.8017666\n",
      "\tspeed: 0.0009s/iter; left time: 13.2762s\n",
      "\titers: 26600, epoch: 1 | loss: 0.8310205\n",
      "\tspeed: 0.0011s/iter; left time: 15.5687s\n",
      "\titers: 26700, epoch: 1 | loss: 0.8445652\n",
      "\tspeed: 0.0013s/iter; left time: 18.5820s\n",
      "\titers: 26800, epoch: 1 | loss: 0.9698565\n",
      "\tspeed: 0.0010s/iter; left time: 13.7504s\n",
      "\titers: 26900, epoch: 1 | loss: 0.9322193\n",
      "\tspeed: 0.0009s/iter; left time: 12.3003s\n",
      "\titers: 27000, epoch: 1 | loss: 0.7658349\n",
      "\tspeed: 0.0010s/iter; left time: 12.8286s\n",
      "\titers: 27100, epoch: 1 | loss: 0.9412658\n",
      "\tspeed: 0.0009s/iter; left time: 12.4581s\n",
      "\titers: 27200, epoch: 1 | loss: 0.9102548\n",
      "\tspeed: 0.0010s/iter; left time: 12.6774s\n",
      "\titers: 27300, epoch: 1 | loss: 0.7385279\n",
      "\tspeed: 0.0010s/iter; left time: 13.3595s\n",
      "\titers: 27400, epoch: 1 | loss: 0.7479113\n",
      "\tspeed: 0.0016s/iter; left time: 21.5028s\n",
      "\titers: 27500, epoch: 1 | loss: 1.0190719\n",
      "\tspeed: 0.0016s/iter; left time: 21.3543s\n",
      "\titers: 27600, epoch: 1 | loss: 0.9985802\n",
      "\tspeed: 0.0016s/iter; left time: 21.2497s\n",
      "\titers: 27700, epoch: 1 | loss: 0.9638859\n",
      "\tspeed: 0.0016s/iter; left time: 21.0595s\n",
      "\titers: 27800, epoch: 1 | loss: 0.8478920\n",
      "\tspeed: 0.0016s/iter; left time: 20.9035s\n",
      "\titers: 27900, epoch: 1 | loss: 0.8914767\n",
      "\tspeed: 0.0012s/iter; left time: 14.6181s\n",
      "\titers: 28000, epoch: 1 | loss: 0.8323877\n",
      "\tspeed: 0.0010s/iter; left time: 12.1496s\n",
      "\titers: 28100, epoch: 1 | loss: 0.9331934\n",
      "\tspeed: 0.0011s/iter; left time: 14.0528s\n",
      "\titers: 28200, epoch: 1 | loss: 0.7427446\n",
      "\tspeed: 0.0017s/iter; left time: 20.3240s\n",
      "\titers: 28300, epoch: 1 | loss: 0.9053704\n",
      "\tspeed: 0.0017s/iter; left time: 20.1386s\n",
      "\titers: 28400, epoch: 1 | loss: 0.8893085\n",
      "\tspeed: 0.0016s/iter; left time: 19.9144s\n",
      "\titers: 28500, epoch: 1 | loss: 0.9067088\n",
      "\tspeed: 0.0011s/iter; left time: 13.4896s\n",
      "\titers: 28600, epoch: 1 | loss: 0.7981566\n",
      "\tspeed: 0.0016s/iter; left time: 18.9807s\n",
      "\titers: 28700, epoch: 1 | loss: 0.9323597\n",
      "\tspeed: 0.0017s/iter; left time: 19.5098s\n",
      "\titers: 28800, epoch: 1 | loss: 0.7452536\n",
      "\tspeed: 0.0017s/iter; left time: 19.3003s\n",
      "\titers: 28900, epoch: 1 | loss: 0.7981821\n",
      "\tspeed: 0.0016s/iter; left time: 19.0975s\n",
      "\titers: 29000, epoch: 1 | loss: 0.8507541\n",
      "\tspeed: 0.0016s/iter; left time: 18.8804s\n",
      "\titers: 29100, epoch: 1 | loss: 1.0513549\n",
      "\tspeed: 0.0016s/iter; left time: 18.6224s\n",
      "\titers: 29200, epoch: 1 | loss: 0.9501761\n",
      "\tspeed: 0.0016s/iter; left time: 18.5457s\n",
      "\titers: 29300, epoch: 1 | loss: 0.7934276\n",
      "\tspeed: 0.0016s/iter; left time: 18.4027s\n",
      "\titers: 29400, epoch: 1 | loss: 0.8175599\n",
      "\tspeed: 0.0016s/iter; left time: 18.1981s\n",
      "\titers: 29500, epoch: 1 | loss: 0.7389849\n",
      "\tspeed: 0.0016s/iter; left time: 18.0346s\n",
      "\titers: 29600, epoch: 1 | loss: 0.8856088\n",
      "\tspeed: 0.0016s/iter; left time: 17.8471s\n",
      "\titers: 29700, epoch: 1 | loss: 0.9260225\n",
      "\tspeed: 0.0013s/iter; left time: 14.3476s\n",
      "\titers: 29800, epoch: 1 | loss: 0.7108463\n",
      "\tspeed: 0.0014s/iter; left time: 14.8387s\n",
      "\titers: 29900, epoch: 1 | loss: 0.7046641\n",
      "\tspeed: 0.0015s/iter; left time: 16.0605s\n",
      "\titers: 30000, epoch: 1 | loss: 0.7289334\n",
      "\tspeed: 0.0015s/iter; left time: 15.2525s\n",
      "\titers: 30100, epoch: 1 | loss: 0.8729213\n",
      "\tspeed: 0.0017s/iter; left time: 17.1959s\n",
      "\titers: 30200, epoch: 1 | loss: 0.8604304\n",
      "\tspeed: 0.0017s/iter; left time: 16.9788s\n",
      "\titers: 30300, epoch: 1 | loss: 0.7703264\n",
      "\tspeed: 0.0016s/iter; left time: 16.7355s\n",
      "\titers: 30400, epoch: 1 | loss: 0.8008189\n",
      "\tspeed: 0.0016s/iter; left time: 16.5753s\n",
      "\titers: 30500, epoch: 1 | loss: 0.9341142\n",
      "\tspeed: 0.0016s/iter; left time: 16.3580s\n",
      "\titers: 30600, epoch: 1 | loss: 0.8346930\n",
      "\tspeed: 0.0016s/iter; left time: 16.1850s\n",
      "\titers: 30700, epoch: 1 | loss: 0.8281370\n",
      "\tspeed: 0.0016s/iter; left time: 16.0230s\n",
      "\titers: 30800, epoch: 1 | loss: 0.8549751\n",
      "\tspeed: 0.0015s/iter; left time: 14.8051s\n",
      "\titers: 30900, epoch: 1 | loss: 0.8198973\n",
      "\tspeed: 0.0015s/iter; left time: 14.2572s\n",
      "\titers: 31000, epoch: 1 | loss: 0.8103421\n",
      "\tspeed: 0.0016s/iter; left time: 15.1470s\n",
      "\titers: 31100, epoch: 1 | loss: 0.7821963\n",
      "\tspeed: 0.0013s/iter; left time: 12.1476s\n",
      "\titers: 31200, epoch: 1 | loss: 0.7436863\n",
      "\tspeed: 0.0017s/iter; left time: 15.3275s\n",
      "\titers: 31300, epoch: 1 | loss: 0.7985000\n",
      "\tspeed: 0.0011s/iter; left time: 10.3067s\n",
      "\titers: 31400, epoch: 1 | loss: 0.9060565\n",
      "\tspeed: 0.0009s/iter; left time: 8.1400s\n",
      "\titers: 31500, epoch: 1 | loss: 0.8037876\n",
      "\tspeed: 0.0009s/iter; left time: 8.2139s\n",
      "\titers: 31600, epoch: 1 | loss: 0.7872249\n",
      "\tspeed: 0.0009s/iter; left time: 7.9477s\n",
      "\titers: 31700, epoch: 1 | loss: 0.8223788\n",
      "\tspeed: 0.0009s/iter; left time: 7.8228s\n",
      "\titers: 31800, epoch: 1 | loss: 0.8112710\n",
      "\tspeed: 0.0009s/iter; left time: 7.7233s\n",
      "\titers: 31900, epoch: 1 | loss: 0.7406722\n",
      "\tspeed: 0.0009s/iter; left time: 7.6679s\n",
      "\titers: 32000, epoch: 1 | loss: 0.7537451\n",
      "\tspeed: 0.0009s/iter; left time: 7.5940s\n",
      "\titers: 32100, epoch: 1 | loss: 0.8811098\n",
      "\tspeed: 0.0016s/iter; left time: 13.3290s\n",
      "\titers: 32200, epoch: 1 | loss: 0.8114384\n",
      "\tspeed: 0.0016s/iter; left time: 13.6079s\n",
      "\titers: 32300, epoch: 1 | loss: 0.8207886\n",
      "\tspeed: 0.0016s/iter; left time: 13.3517s\n",
      "\titers: 32400, epoch: 1 | loss: 0.9180136\n",
      "\tspeed: 0.0016s/iter; left time: 13.2353s\n",
      "\titers: 32500, epoch: 1 | loss: 0.9330342\n",
      "\tspeed: 0.0011s/iter; left time: 8.7775s\n",
      "\titers: 32600, epoch: 1 | loss: 0.6821860\n",
      "\tspeed: 0.0009s/iter; left time: 6.8924s\n",
      "\titers: 32700, epoch: 1 | loss: 0.9067538\n",
      "\tspeed: 0.0009s/iter; left time: 7.1220s\n",
      "\titers: 32800, epoch: 1 | loss: 0.6853381\n",
      "\tspeed: 0.0009s/iter; left time: 6.8587s\n",
      "\titers: 32900, epoch: 1 | loss: 0.8292158\n",
      "\tspeed: 0.0009s/iter; left time: 6.6686s\n",
      "\titers: 33000, epoch: 1 | loss: 0.9124919\n",
      "\tspeed: 0.0009s/iter; left time: 6.7397s\n",
      "\titers: 33100, epoch: 1 | loss: 0.6863363\n",
      "\tspeed: 0.0010s/iter; left time: 7.1786s\n",
      "\titers: 33200, epoch: 1 | loss: 1.0023669\n",
      "\tspeed: 0.0013s/iter; left time: 9.1471s\n",
      "\titers: 33300, epoch: 1 | loss: 0.8647565\n",
      "\tspeed: 0.0015s/iter; left time: 10.5427s\n",
      "\titers: 33400, epoch: 1 | loss: 0.6776915\n",
      "\tspeed: 0.0015s/iter; left time: 10.8376s\n",
      "\titers: 33500, epoch: 1 | loss: 0.8535874\n",
      "\tspeed: 0.0010s/iter; left time: 6.6702s\n",
      "\titers: 33600, epoch: 1 | loss: 1.1169449\n",
      "\tspeed: 0.0009s/iter; left time: 6.5128s\n",
      "\titers: 33700, epoch: 1 | loss: 0.8398063\n",
      "\tspeed: 0.0009s/iter; left time: 6.2678s\n",
      "\titers: 33800, epoch: 1 | loss: 0.7333391\n",
      "\tspeed: 0.0009s/iter; left time: 5.9760s\n",
      "\titers: 33900, epoch: 1 | loss: 0.8516861\n",
      "\tspeed: 0.0009s/iter; left time: 5.7845s\n",
      "\titers: 34000, epoch: 1 | loss: 0.8481386\n",
      "\tspeed: 0.0009s/iter; left time: 5.6812s\n",
      "\titers: 34100, epoch: 1 | loss: 0.6932484\n",
      "\tspeed: 0.0009s/iter; left time: 5.6102s\n",
      "\titers: 34200, epoch: 1 | loss: 1.0021070\n",
      "\tspeed: 0.0011s/iter; left time: 7.0383s\n",
      "\titers: 34300, epoch: 1 | loss: 0.7814626\n",
      "\tspeed: 0.0011s/iter; left time: 6.8239s\n",
      "\titers: 34400, epoch: 1 | loss: 0.7467374\n",
      "\tspeed: 0.0011s/iter; left time: 6.7660s\n",
      "\titers: 34500, epoch: 1 | loss: 0.9059880\n",
      "\tspeed: 0.0015s/iter; left time: 9.1712s\n",
      "\titers: 34600, epoch: 1 | loss: 1.0214373\n",
      "\tspeed: 0.0016s/iter; left time: 9.6642s\n",
      "\titers: 34700, epoch: 1 | loss: 0.7187406\n",
      "\tspeed: 0.0013s/iter; left time: 7.8012s\n",
      "\titers: 34800, epoch: 1 | loss: 0.8794240\n",
      "\tspeed: 0.0015s/iter; left time: 8.3048s\n",
      "\titers: 34900, epoch: 1 | loss: 0.8065582\n",
      "\tspeed: 0.0016s/iter; left time: 9.1635s\n",
      "\titers: 35000, epoch: 1 | loss: 0.9363002\n",
      "\tspeed: 0.0016s/iter; left time: 8.9274s\n",
      "\titers: 35100, epoch: 1 | loss: 0.8566724\n",
      "\tspeed: 0.0015s/iter; left time: 8.1476s\n",
      "\titers: 35200, epoch: 1 | loss: 0.6745476\n",
      "\tspeed: 0.0017s/iter; left time: 8.8446s\n",
      "\titers: 35300, epoch: 1 | loss: 0.8302051\n",
      "\tspeed: 0.0017s/iter; left time: 8.6307s\n",
      "\titers: 35400, epoch: 1 | loss: 0.7488455\n",
      "\tspeed: 0.0017s/iter; left time: 8.4261s\n",
      "\titers: 35500, epoch: 1 | loss: 1.0175737\n",
      "\tspeed: 0.0017s/iter; left time: 8.2674s\n",
      "\titers: 35600, epoch: 1 | loss: 0.8215101\n",
      "\tspeed: 0.0015s/iter; left time: 7.3324s\n",
      "\titers: 35700, epoch: 1 | loss: 0.6785465\n",
      "\tspeed: 0.0013s/iter; left time: 6.2535s\n",
      "\titers: 35800, epoch: 1 | loss: 0.9721969\n",
      "\tspeed: 0.0016s/iter; left time: 7.6046s\n",
      "\titers: 35900, epoch: 1 | loss: 0.8683612\n",
      "\tspeed: 0.0012s/iter; left time: 5.5447s\n",
      "\titers: 36000, epoch: 1 | loss: 0.6779696\n",
      "\tspeed: 0.0016s/iter; left time: 7.2592s\n",
      "\titers: 36100, epoch: 1 | loss: 0.9242420\n",
      "\tspeed: 0.0016s/iter; left time: 7.2013s\n",
      "\titers: 36200, epoch: 1 | loss: 0.8495383\n",
      "\tspeed: 0.0016s/iter; left time: 6.9328s\n",
      "\titers: 36300, epoch: 1 | loss: 0.8964359\n",
      "\tspeed: 0.0016s/iter; left time: 6.8822s\n",
      "\titers: 36400, epoch: 1 | loss: 0.5762485\n",
      "\tspeed: 0.0017s/iter; left time: 6.7654s\n",
      "\titers: 36500, epoch: 1 | loss: 0.7449995\n",
      "\tspeed: 0.0015s/iter; left time: 5.9287s\n",
      "\titers: 36600, epoch: 1 | loss: 0.7523981\n",
      "\tspeed: 0.0017s/iter; left time: 6.4464s\n",
      "\titers: 36700, epoch: 1 | loss: 0.8803532\n",
      "\tspeed: 0.0017s/iter; left time: 6.2772s\n",
      "\titers: 36800, epoch: 1 | loss: 0.7219741\n",
      "\tspeed: 0.0015s/iter; left time: 5.6648s\n",
      "\titers: 36900, epoch: 1 | loss: 0.6892194\n",
      "\tspeed: 0.0017s/iter; left time: 5.9262s\n",
      "\titers: 37000, epoch: 1 | loss: 0.8587859\n",
      "\tspeed: 0.0015s/iter; left time: 5.3321s\n",
      "\titers: 37100, epoch: 1 | loss: 0.7903273\n",
      "\tspeed: 0.0016s/iter; left time: 5.5491s\n",
      "\titers: 37200, epoch: 1 | loss: 0.6506434\n",
      "\tspeed: 0.0013s/iter; left time: 4.1427s\n",
      "\titers: 37300, epoch: 1 | loss: 0.7453101\n",
      "\tspeed: 0.0009s/iter; left time: 2.9176s\n",
      "\titers: 37400, epoch: 1 | loss: 0.8727921\n",
      "\tspeed: 0.0009s/iter; left time: 2.7476s\n",
      "\titers: 37500, epoch: 1 | loss: 0.8591213\n",
      "\tspeed: 0.0009s/iter; left time: 2.6582s\n",
      "\titers: 37600, epoch: 1 | loss: 0.8699015\n",
      "\tspeed: 0.0009s/iter; left time: 2.5940s\n",
      "\titers: 37700, epoch: 1 | loss: 0.9815041\n",
      "\tspeed: 0.0009s/iter; left time: 2.4973s\n",
      "\titers: 37800, epoch: 1 | loss: 0.8533180\n",
      "\tspeed: 0.0009s/iter; left time: 2.5470s\n",
      "\titers: 37900, epoch: 1 | loss: 0.6959220\n",
      "\tspeed: 0.0015s/iter; left time: 3.9849s\n",
      "\titers: 38000, epoch: 1 | loss: 0.9210864\n",
      "\tspeed: 0.0014s/iter; left time: 3.5671s\n",
      "\titers: 38100, epoch: 1 | loss: 0.8200228\n",
      "\tspeed: 0.0016s/iter; left time: 3.9332s\n",
      "\titers: 38200, epoch: 1 | loss: 0.7688267\n",
      "\tspeed: 0.0016s/iter; left time: 3.7551s\n",
      "\titers: 38300, epoch: 1 | loss: 0.8592523\n",
      "\tspeed: 0.0016s/iter; left time: 3.3999s\n",
      "\titers: 38400, epoch: 1 | loss: 0.9378232\n",
      "\tspeed: 0.0015s/iter; left time: 3.1355s\n",
      "\titers: 38500, epoch: 1 | loss: 0.8113896\n",
      "\tspeed: 0.0016s/iter; left time: 3.2722s\n",
      "\titers: 38600, epoch: 1 | loss: 0.8388217\n",
      "\tspeed: 0.0017s/iter; left time: 3.1211s\n",
      "\titers: 38700, epoch: 1 | loss: 0.6375325\n",
      "\tspeed: 0.0016s/iter; left time: 2.9422s\n",
      "\titers: 38800, epoch: 1 | loss: 0.7812521\n",
      "\tspeed: 0.0016s/iter; left time: 2.7694s\n",
      "\titers: 38900, epoch: 1 | loss: 0.9696508\n",
      "\tspeed: 0.0016s/iter; left time: 2.6090s\n",
      "\titers: 39000, epoch: 1 | loss: 0.8676490\n",
      "\tspeed: 0.0016s/iter; left time: 2.4389s\n",
      "\titers: 39100, epoch: 1 | loss: 0.8420278\n",
      "\tspeed: 0.0016s/iter; left time: 2.2761s\n",
      "\titers: 39200, epoch: 1 | loss: 0.8587189\n",
      "\tspeed: 0.0016s/iter; left time: 2.1143s\n",
      "\titers: 39300, epoch: 1 | loss: 1.0704151\n",
      "\tspeed: 0.0013s/iter; left time: 1.5103s\n",
      "\titers: 39400, epoch: 1 | loss: 0.8426393\n",
      "\tspeed: 0.0012s/iter; left time: 1.2658s\n",
      "\titers: 39500, epoch: 1 | loss: 0.7414870\n",
      "\tspeed: 0.0015s/iter; left time: 1.4571s\n",
      "\titers: 39600, epoch: 1 | loss: 0.7378505\n",
      "\tspeed: 0.0010s/iter; left time: 0.8449s\n",
      "\titers: 39700, epoch: 1 | loss: 0.9442355\n",
      "\tspeed: 0.0009s/iter; left time: 0.7492s\n",
      "\titers: 39800, epoch: 1 | loss: 0.6310609\n",
      "\tspeed: 0.0010s/iter; left time: 0.6968s\n",
      "\titers: 39900, epoch: 1 | loss: 0.8223770\n",
      "\tspeed: 0.0013s/iter; left time: 0.7822s\n",
      "\titers: 40000, epoch: 1 | loss: 0.9958009\n",
      "\tspeed: 0.0016s/iter; left time: 0.8028s\n",
      "\titers: 40100, epoch: 1 | loss: 0.7037624\n",
      "\tspeed: 0.0016s/iter; left time: 0.6386s\n",
      "\titers: 40200, epoch: 1 | loss: 0.8164358\n",
      "\tspeed: 0.0016s/iter; left time: 0.4757s\n",
      "\titers: 40300, epoch: 1 | loss: 0.6159351\n",
      "\tspeed: 0.0016s/iter; left time: 0.3104s\n",
      "\titers: 40400, epoch: 1 | loss: 0.8428268\n",
      "\tspeed: 0.0016s/iter; left time: 0.1465s\n",
      "Epoch: 1 cost time: 51.65245723724365\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m cost time: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m epoch_time))\n\u001b[1;32m     45\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(train_loss)\n\u001b[0;32m---> 47\u001b[0m vali_mse, vali_mae \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mltboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvali_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m test_mse, test_mae \u001b[38;5;241m=\u001b[39m validate(ltboost, test_loader, criterion)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m, Steps: \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{2:.7f}\u001b[39;00m\u001b[38;5;124m Vali Loss: \u001b[39m\u001b[38;5;132;01m{3:.7f}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{4:.7f}\u001b[39;00m\u001b[38;5;124m Test Loss: \u001b[39m\u001b[38;5;132;01m{5:.7f}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{6:.7f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     50\u001b[0m     epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, train_steps, train_loss, vali_mse, vali_mae, test_mse, test_mae))\n",
      "Cell \u001b[0;32mIn[8], line 73\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(model, vali_loader, criterion)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m vali_loader:\n\u001b[1;32m     72\u001b[0m     batch_x \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> 73\u001b[0m     batch_y \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_y\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mPRED_LEN\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     74\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(batch_x)\n\u001b[1;32m     75\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs[:, \u001b[38;5;241m-\u001b[39mPRED_LEN:,:]\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "PATIENCE = 3\n",
    "model_optim = optim.Adam(ltboost.parameters(), lr=LR)\n",
    "criterion = nn.L1Loss()\n",
    "ltboost = ltboost.to(DEVICE)\n",
    "\n",
    "\n",
    "print(\"======= Training of linear part starting... =======\")\n",
    "time_now = time.time()\n",
    "train_steps = len(train_loader)\n",
    "best_val_loss, final_mse, final_mae, rounds_count = float('inf'), 0, 0, 0\n",
    "for epoch in range(EPOCHS):\n",
    "    iter_count = 0\n",
    "    train_loss = []\n",
    "\n",
    "    epoch_time = time.time()\n",
    "    for i, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        iter_count += 1\n",
    "        model_optim.zero_grad()\n",
    "        batch_x = batch_x.float().to(DEVICE)\n",
    "        batch_y = batch_y.float().to(DEVICE)\n",
    "    \n",
    "        outputs = ltboost(batch_x)\n",
    "        outputs = outputs[:, -ltboost.pred_len:, :]\n",
    "        batch_y = batch_y[:, -ltboost.pred_len:].unsqueeze(-1).to(DEVICE)  # Adjusted indexing\n",
    "    \n",
    "        # Print shapes for debugging\n",
    "        #print(f\"outputs shape: {outputs.shape}\")\n",
    "        #print(f\"batch_y shape: {batch_y.shape}\")\n",
    "    \n",
    "        loss = criterion(outputs, batch_y)\n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "            speed = (time.time() - time_now) / iter_count\n",
    "            left_time = speed * ((EPOCHS - epoch) * train_steps - i)\n",
    "            print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "            iter_count = 0\n",
    "            time_now = time.time()\n",
    "    \n",
    "        loss.backward()\n",
    "        model_optim.step()\n",
    "    print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "    train_loss = np.average(train_loss)\n",
    "\n",
    "    vali_mse, vali_mae = validate(ltboost, vali_loader, criterion)\n",
    "    test_mse, test_mae = validate(ltboost, test_loader, criterion)\n",
    "    print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f}/{4:.7f} Test Loss: {5:.7f}/{6:.7f}\".format(\n",
    "        epoch + 1, train_steps, train_loss, vali_mse, vali_mae, test_mse, test_mae))\n",
    "\n",
    "    if vali_mae < best_val_loss:\n",
    "        best_val_loss = vali_mae\n",
    "        final_mse, final_mae = test_mse, test_mae\n",
    "        rounds_count = 0 \n",
    "    else:\n",
    "        rounds_count +=1\n",
    "\n",
    "    print(f'EarlyStopping counter: {rounds_count} out of {PATIENCE}')\n",
    "    if rounds_count == PATIENCE:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    lr_adjust = {epoch: LR * (0.5 ** (epoch // 1))}\n",
    "    lr = lr_adjust[epoch]\n",
    "    for param_group in model_optim.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    print('Updating learning rate to {}'.format(lr))\n",
    "\n",
    "print('======= Final test loss (MSE/MAE): {0:.7f}/{1:.7f} ======='.format(final_mse, final_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c870cfe2-a7a3-4694-8139-3d5756598321",
   "metadata": {},
   "source": [
    "# Train LGBM part\n",
    "# Get full dataset for LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5201b5-0a96-4292-8f41-a2a0fff8f282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start LGBM Training\n",
      "Initial X shape: torch.Size([1295616, 104, 4])\n",
      "Initial y shape: torch.Size([1295616, 60])\n",
      "y after unsqueeze and expand shape: torch.Size([1295616, 60, 4])\n",
      "y after subtraction shape: torch.Size([1295616, 60, 4])\n",
      "output shape: torch.Size([1295616, 60, 4])\n",
      "y after final subtraction shape: torch.Size([1295616, 60, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_batches = []\n",
    "y_batches = []\n",
    "\n",
    "for batch_X, batch_y in train_loader:\n",
    "    X_batches.append(batch_X.float())\n",
    "    y_batches.append(batch_y[:, -PRED_LEN:].float())  # Adjusted indexing\n",
    "\n",
    "X, y = torch.cat(X_batches, dim=0), torch.cat(y_batches, dim=0)\n",
    "\n",
    "# Train LGBM\n",
    "\n",
    "# Train LGBM\n",
    "time_start = time.time()\n",
    "print(\"Start LGBM Training\")\n",
    "ltboost.train(X, y)\n",
    "time_train = time.time()\n",
    "\n",
    "print(\"Predicting\")\n",
    "outputs = ltboost.predict(X)\n",
    "time_predict = time.time()\n",
    "outputs = outputs.detach().cpu().numpy()\n",
    "y = y.detach().cpu().numpy()\n",
    "errs = np.abs(outputs - y)\n",
    "train_mse, train_mae = np.mean(np.array(errs)**2), np.mean(errs)\n",
    "\n",
    "print(\"Training time: {0:.2f}\".format(time_train - time_start))\n",
    "print(\"(Train) Prediction time: {0:.2f}\".format(time_predict - time_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d898ed-1955-4b26-ba3f-262a15146bed",
   "metadata": {},
   "source": [
    "# Evaluate LTBoost on val and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e9664-41e1-44e8-80f7-5ccf5c592682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vali(model, d_loader):\n",
    "\n",
    "    X_batches, y_batches = [], [],\n",
    "    for batch_X, batch_y in d_loader:\n",
    "        X_batches.append(batch_X.float())\n",
    "        y_batches.append(batch_y[:, -PRED_LEN:,:].float())\n",
    "    X, y = torch.cat(X_batches, dim=0), torch.cat(y_batches, dim=0)\n",
    "\n",
    "    outputs = model.predict(X)\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    y = y.numpy()\n",
    "    errs = np.abs(y - outputs)\n",
    "    mae, mse,= np.mean(np.array(errs)**2), np.mean(errs)\n",
    "    return mse, mae\n",
    "\n",
    "\n",
    "vali_mae, vali_mse = vali(ltboost, vali_loader)\n",
    "test_mae, test_mse = vali(ltboost, test_loader)\n",
    "print(\"(mse/mae) Train: {0:.7f}/{1:.7f} Vali: {2:.7f}/{3:.7f} Test: {4:.7f}/{5:.7f}\".format(\n",
    "                train_mse, train_mae, vali_mse, vali_mae, test_mse, test_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f08a5-008f-455e-aed3-b993284fbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790d8fbb-e6b0-45bb-a7e2-63eb76bfae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch, y_batch = next(iter(test_loader))\n",
    "with torch.no_grad():\n",
    "    lin_preds = ltboost(X_batch.float())\n",
    "lt_preds = ltboost.predict(X_batch)\n",
    "\n",
    "gt = np.concatenate((X_batch[10,:,-1], y_batch[10,-PRED_LEN:,-1]), axis=0)\n",
    "plt.figure()\n",
    "plt.plot(list(range(-SEQ_LEN, PRED_LEN)), gt, label='GroundTruth', linewidth=1.5)\n",
    "plt.plot(list(range(PRED_LEN)), lin_preds[10,:,-1], label='Linear part', color='orange', linewidth=1)\n",
    "plt.plot(list(range(PRED_LEN)), lt_preds[10,:,-1], label='LTBoost', color='red', linewidth=1)\n",
    "plt.axvline(x=0, color=\"k\")\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title('ILI sample prediction')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40e90a-0088-4d37-9e2a-6a65f340b692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
